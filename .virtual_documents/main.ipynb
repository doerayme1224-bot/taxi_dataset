


import pandas as pd
import seaborn as sns
import numpy as np
import missingno as msno
import matplotlib.pyplot as plt
import pickle

from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Lasso
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import root_mean_squared_error
from sklearn.metrics import mean_absolute_percentage_error





df = sns.load_dataset('taxis')


df.info()





sub = df.dropna()
# decided to experiment with dropping columns, to see if it was too many


sub.info()
# did not seem to drop many so I kept it


borough_val = ['Manhattan', 'Queens', 'Brooklyn', 'Bronx', 'Staten Island']
borough_dum = [1, 2, 3, 4, 5]


sub['pickup_borough'] = sub['pickup_borough'].replace(to_replace= borough_val, value= borough_dum)
sub['dropoff_borough'] = sub['dropoff_borough'].replace(to_replace= borough_val, value= borough_dum)

# used the replace method to change strings to numbers
# assigned to variables, one with the actual values, and one with what I wanted to replace those values with
# saved changes that I made to that column


sub['color'] = pd.Categorical(sub['color'], categories = ['yellow', 'green'], ordered = True)
sub['payment'] = pd.Categorical(sub['payment'], categories= ['credit card', 'cash'], ordered = True)


sub['color'] = sub['color'].cat.codes
sub['payment'] = sub['payment'].cat.codes

# used the categorical method for these two columns
# changes the string value types to categorical ones by using pd.Categorical
# used .cat.codes to change the categorical data types to numerical ones


sub_2 = sub.drop(columns = ['pickup_zone','dropoff_zone'], axis = 1)
# dropped tehse columns, cause using them for a flask app would be confusing


sub_2.info()





sub_2.to_pickle('data/cleaned_taxis_dataset')
# pickled the dataset as I wanted to keep the datetime objects as is (in case they were useful)





data = pd.read_pickle('data/cleaned_taxis_dataset')


data.head()


data.info()





sns.set_theme('paper')
sns.set_style('darkgrid')


plt.figure(figsize=(15,10))
sns.heatmap(data.corr()[['distance']].sort_values(by = 'distance', ascending = False), 
           annot = True, 
           vmin = -1, 
           vmax = 1, 
           cmap = 'coolwarm')
plt.title('Correlation of Target and Features')
plt.xlabel('Target')
plt.ylabel('Features')
plt.savefig('visuals/heatmap.png')
# heatmap I used to find which features could be used to predict my target (I decided on distance)


sns.histplot(data, 
            x = 'distance', 
            binwidth= 1.25)
plt.title('Distribution of Distance Traveled')
plt.xlabel('Distance of Trip (in Miles)')
plt.ylabel('Number of Trips')
plt.savefig('visuals/histogram.png')
# maded a histogram to show the distribution of the average trip distances, as that was my target


sns.regplot(data, x = 'distance', y = 'tip', line_kws={'color' : 'orange'})
plt.title('Relationship Between Distance, and Tips')
plt.xlabel('Distance of Trip (in Miles)')
plt.ylabel('Tip Amount (in USD)')
plt.savefig('visuals/regplot_tip.png')
# made a regplot to show one of the relationships between distance and trips. shows us that while there does seem to be some correlation, most people seem to be either to generous or to skimpy, heavily affecting the reg line


sns.regplot(data, x = 'distance', y = 'total', line_kws={'color' : 'orange'})
plt.title('Relationship Between Distance, and Total')
plt.xlabel('Distance of Trip (in Miles)')
plt.ylabel('Total Trip Cost (in USD)')
plt.savefig('visuals/regplot_total.png')
# reg plot showing the relationship between distance and total cost. much stronger relationship[, you could assume that an expensive taxi ride meant a lobger distance traveled





X = data[['fare','tip','tolls','total']]

y = data['distance']

# features and target





scores = []

for i in range(10, 31, 1):
    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size = i/100)
    lr = LinearRegression()
    lr.fit(X_train, y_train)
    train_score = lr.score(X_train, y_train)
    test_score = lr.score(X_test, y_test)

    scores.append({'test_size': i, 'train_score': train_score, 'test_score': test_score})

df_scores = pd.DataFrame(scores)

# for loop that scores many different testsizes for a linear regression problem. appends the eval metrics onto a empty list, and turns it intop a dataframe for easy viewing


# data frame in question
df_scores
# test size of 30 poercent seems to be the best


X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size = 0.30)
# making the lr model with the best test size


lr = LinearRegression()


lr.fit(X_train, y_train)
# fitting the model





lr.score(X_test, y_test)





y_preds = lr.predict(X_test)
root_mean_squared_error(y_test, y_preds)
# ecaluating the rmse, this one did well


baseline_preds = np.full_like(y_test, y_test.mean())
root_mean_squared_error(y_test, baseline_preds)














scores = []

for i in range(10, 31, 1):
    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size = i/100)
    las = Lasso()
    las.fit(X_train, y_train)
    train_score = las.score(X_train, y_train)
    test_score = las.score(X_test, y_test)

    scores.append({'test_size': i, 'train_score': train_score, 'test_score': test_score})

df_scores = pd.DataFrame(scores)


df_scores
# test size of 30 percet is also the berst for this one











scores = []

for i in range(10, 31, 1):
    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size = i/100)
    rfr = RandomForestRegressor(random_state= 42)
    rfr.fit(X_train, y_train)
    train_score = rfr.score(X_train, y_train)
    test_score = rfr.score(X_test, y_test)
    
    scores.append({'test_size': i, 'train_score': train_score, 'test_score': test_score})

df_scores = pd.DataFrame(scores)


df_scores
# test size 30 strikes again


X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size = 0.30)


rfr = RandomForestRegressor(random_state= 42)


rfr.fit(X_train, y_train)





rfr.score(X_test, y_test)





y_preds = rfr.predict(X_test)
root_mean_squared_error(y_test, y_preds)


baseline_preds = np.full_like(y_test, y_test.mean())
root_mean_squared_error(y_test, baseline_preds)














scores = []

for i in range(10, 31, 1):
    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size = i/100)
    dtr = DecisionTreeRegressor(random_state = 42)
    dtr.fit(X_train, y_train)
    train_score = dtr.score(X_train, y_train)
    test_score = dtr.score(X_test, y_test)
    
    scores.append({'test_size': i, 'train_score': train_score, 'test_score': test_score})

df_scores = pd.DataFrame(scores)



df_scores
# test size 27








def model_factory(model, columns):
    """
    Creator: Daniel
    inputs: 
        model: The machine learning model object to use in the pipeline
        columns: the list of columns to use for scaling
    outputs:
        returns the model, ready to be fitted and used. 
    """
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', StandardScaler(), columns)
        ],
        remainder='passthrough'
    )
    # this preprocessor will scale the columns passed in. Any other columns will go through without being scaled. 
    model = Pipeline(
        steps=[
            ('preprocess', preprocessor),
            ('model', model)
        ]
    )
    # this is the pipeline the model will use, so it will first scale it with the preprocessor, then run it through the model. 
    # this will make it easier to use, as it will scale the inputs automatically, so it doesn't need to be scaled
    # outside of the model. 
    return model

# function creates a pipline that automatically scales values for you


scores = []

for k in range(3, 32, 2):
    knn = KNeighborsRegressor(n_neighbors = k)
    knn.fit(X_train_sc, y_train)
    train_score = knn.score(X_train_sc, y_train)
    test_score = knn.score(X_test_sc, y_test)

    scores.append({'k': k, 'train_score': train_score, 'test_score': test_score})

df_scores = pd.DataFrame(scores)


df_scores
# k of 19


X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size=0.30)





knn = model_factory(KNeighborsRegressor(n_neighbors = 19), X.columns)
# filling the model_factory parameters with the model that performed well and the name of the


knn.fit(X_train, y_train)





knn.score(X_test, y_test)





y_preds = knn.predict(X_test)


root_mean_squared_error(y_test, y_preds)


baseline_preds = np.full_like(y_test, y_test.mean())
root_mean_squared_error(y_test, baseline_preds)





knn_pkl = ''


with open('knn_pkl', 'wb') as file:
    model = pickle.dump(knn, file)


with open('knn_pkl', 'rb') as file:
    model = pickle.load(file)
